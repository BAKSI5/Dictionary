{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14544298",
   "metadata": {},
   "source": [
    "# Data wrangling<a id='2_Data_wrangling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef2753",
   "metadata": {},
   "source": [
    "## Introduction<a id='2.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55996777",
   "metadata": {},
   "source": [
    "In today's interconnected world, effective language translation plays a crucial role in breaking down communication barriers and enabling cross-cultural understanding. An automated translation system can facilitate seamless interactions, enhance information dissemination, and support users in understanding content in their preferred language.\n",
    "\n",
    "For this project, I have chosen to fine-tune a pretrained model found on https://huggingface.co/tasks/translation that enables accurate and contextually relevant translation from English to Russian. The goal is to enhance the accessibility of information across language barriers and improve communication between users who speak different languages.\n",
    "\n",
    "Utilizing a pretrained model offers substantial advantages. It diminishes computational expenses, lessens your environmental impact, and grants you access to cutting-edge models without the need to initiate training from the ground up. Transformers offer an array of thousands of pretrained models catering to various tasks. Upon employing a pretrained model, you fine-tune it using a dataset tailored to your specific task, a technique recognized as fine-tuning, which wields remarkable training prowess.\n",
    "\n",
    "The model has been trained on a dataset sourced from Kaggle, encompassing pairs of concise English and Russian sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e37fb4",
   "metadata": {},
   "source": [
    "## 1. Data Collection and Overview<a id='2.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88751bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from string import punctuation\n",
    "import random\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06da173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/bayar/Downloads/Capstone3/rus.txt',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175aae07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Марш!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Иди.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Идите.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Здравствуйте.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Привет!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0              1                                                  2\n",
       "0  Go.          Марш!  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1  Go.           Иди.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2  Go.         Идите.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3  Hi.  Здравствуйте.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "4  Hi.        Привет!  CC-BY 2.0 (France) Attribution: tatoeba.org #5..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769de35",
   "metadata": {},
   "source": [
    "## 2. Data Definition and Cleaning<a id='2.6_Explore_The_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7dbb554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Марш!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Иди.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Идите.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Здравствуйте.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Привет!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0              1\n",
       "0  Go.          Марш!\n",
       "1  Go.           Иди.\n",
       "2  Go.         Идите.\n",
       "3  Hi.  Здравствуйте.\n",
       "4  Hi.        Привет!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep first two columns as the last column is not informative\n",
    "data=data.iloc[:,:2]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b07c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363386, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55eb26a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 363386 entries, 0 to 363385\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       363386 non-null  object\n",
      " 1   1       363386 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a7b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting every letter to lower case\n",
    "data[0] = data[0].apply(lambda x: str(x).lower())\n",
    "data[1] = data[1].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8159c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing apostrophe from the sentences\n",
    "data[0] = data[0].apply(lambda x: re.sub(\"'\",\"\",x))\n",
    "data[1] = data[1].apply(lambda x: re.sub(\"'\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ee2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "# removing all the punctuations\n",
    "data[0] = data[0].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "data[1] = data[1].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1733628b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>марш</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>иди</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go</td>\n",
       "      <td>идите</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi</td>\n",
       "      <td>здравствуйте</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi</td>\n",
       "      <td>привет</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0             1\n",
       "0  go          марш\n",
       "1  go           иди\n",
       "2  go         идите\n",
       "3  hi  здравствуйте\n",
       "4  hi        привет"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b00b59",
   "metadata": {},
   "source": [
    "## 3. Tokenization<a id='2.6_Explore_The_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b661003",
   "metadata": {},
   "source": [
    "Within the realm of natural language processing (NLP), tokenization stands as the pivotal procedure for disintegrating text into discrete units referred to as tokens. While these tokens commonly constitute words, they have the flexibility to encompass phrases, subwords, or even characters, contingent on the specific application. Tokenization holds a foundational role across numerous NLP undertakings, encompassing language modeling, machine translation, and text classification. Subsequent to the tokenization process, the text can undergo conversion into a numerical format, which then serves as input for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bff2f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c01a0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tokenizers\n",
    "\n",
    "english_tokenizer = data[0]\n",
    "russian_tokenizer = data[1]\n",
    "\n",
    "english_tokenizer = Tokenizer()\n",
    "russian_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizers on text data\n",
    "english_tokenizer.fit_on_texts(data[0])\n",
    "russian_tokenizer.fit_on_texts(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9509c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in the English vocabulary: 16697\n",
      "The number of words in the Russian vocabulary: 54025\n"
     ]
    }
   ],
   "source": [
    "word_index = english_tokenizer.word_index\n",
    "print(f\"The number of words in the English vocabulary: {len(word_index)}\")\n",
    "\n",
    "word_index_ru = russian_tokenizer.word_index\n",
    "print(f\"The number of words in the Russian vocabulary: {len(word_index_ru)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0c03d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text sequences to integer sequences\n",
    "english_sequences = english_tokenizer.texts_to_sequences(data[0])\n",
    "russian_sequences = russian_tokenizer.texts_to_sequences(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344d4fa",
   "metadata": {},
   "source": [
    "## 4. Padding<a id='2.6_Explore_The_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55870fa4",
   "metadata": {},
   "source": [
    "A consistent length for input sequences is frequently necessary. When input sequences vary in length, it becomes essential to pad them using a designated placeholder value (typically 0) to standardize their lengths. This padding procedure guarantees uniform input sizes for the model, a prerequisite for effective training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "568e755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 10  # Example length\n",
    "padded_english_sequences = pad_sequences(english_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "padded_russian_sequences = pad_sequences(russian_sequences, maxlen=max_sequence_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4be42d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Sequence Lengths:\n",
      "Row 1: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 2: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 3: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 4: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 5: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 6: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 7: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 8: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 9: Sequence Lengths - Column 0: 10, Column 1: 10\n",
      "Row 10: Sequence Lengths - Column 0: 10, Column 1: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary of Sequence Lengths:\")\n",
    "for i in range(10):\n",
    "    print(f\"Row {i + 1}: Sequence Lengths - Column 0: {len(padded_english_sequences[i])}, Column 1: {len(padded_russian_sequences[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80167256",
   "metadata": {},
   "source": [
    "## Summary<a id='2.6_Explore_The_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced86f8",
   "metadata": {},
   "source": [
    "Our initial steps involved transforming all characters to lowercase and eliminating any punctuation. Subsequently, we carried out text preprocessing using a tokenizer. This tool segments text into tokens based on specified rules. These tokens are then translated into numerical values and ultimately into tensors, which serve as inputs for the model. If the model demands supplementary inputs, the tokenizer integrates them.\n",
    "\n",
    "The concluding phase encompassed the transformation of sentences into a consistent format. To achieve this, we employed a Padding strategy, which ensures that tensors are uniform in shape by introducing a distinctive padding token into shorter sentences.\n",
    "\n",
    "With these preparations complete, the dataset stands poised for utilization with a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52caa44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
